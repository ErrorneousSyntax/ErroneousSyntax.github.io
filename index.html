<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dimensionality Reduction in Clustering</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 40px;
            background-color: #f9f9f9;
            color: #333;
        }
        .content {
            max-width: 800px;
            margin: auto;
            padding: 20px;
            background-color: #fff;
            border-radius: 10px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        h1 {
            text-align: center;
            color: #2c3e50;
        }
    </style>
</head>
<body>
    <div class="content">
        <h1>Applications of Gradient Descent: <strong>t-SNE Dimensionality Reduction</strong></h1>
        <h2>Introduction</h2>
        <p>
           This is an introductory article aimed at engineers who have a curiosity in where <strong>engineering maths</strong> (such as <strong>multivariate calculus</strong>) is used in rigorous mathematical research. Particularly for <strong>Design Engineers</strong>, <strong>dimensionality reduction</strong> will be crucial to analyse complex data and gather <strong>true hidden insights</strong>.
        </p>
        <p>
            <strong>High-Dimensional data</strong>, which is data with many features, is becoming more and more prevalent in fields like <strong>machine learning</strong>, <strong>computer vision</strong>, and <strong>finance</strong>. These large datasets can provide <strong>invaluable insights</strong>, but humans are limited to visualising and identifying patterns in <strong>3 dimensions</strong>. This is where <strong>dimensionality reduction</strong> comes in.
        </p>
        <p>
            Dimensionality reduction techniques <strong>project data points</strong> from high-dimensional space into lower-dimensional spaces, while retaining important relationships and <strong>clusters</strong> between points.
        </p>
        <p>
            Many <strong>linear</strong> dimensionality reduction techniques exist, the most popular of which is <strong>Principal Component Analysis (PCA)</strong>, which relies on <strong>linear algebra</strong> to identify key axes (principal components) of variance. 
        </p>
        <p>
            However, linear techniques become less accurate when the data is <strong>non-linear</strong> (i.e., relationships cannot be explained by a simple line), so special techniques such as <strong>t-SNE</strong>, based on <strong>manifold learning</strong>, become necessary.
        </p>
    </div>

    <div class="content">
        <h2>3D Visualization of 15 Clusters</h2>
        <p>
            First, we will understand why <strong>dimensionality reduction</strong> is important for finding cluster relationships. We start with a simple <strong>3D graph</strong> of artificial clusters.
            *Hold and rotate the graph to see the full data in 3D.
        </p>
        <iframe src="plot_3d.html" width="100%" height="500" style="border:none;"></iframe>
        <p>
            To simulate a rudimentary dimensionality reduction, imagine viewing this graph on a <strong>2D screen</strong>. There are overlapping clusters and the "depth" distance between clusters is not preserved. 
        </p>
        <p>
            We need a more robust algorithm that embeds high-D data accurately while preserving the <strong>topology</strong> of the data. This type of dimensionality reduction is called <strong>manifold learning</strong>.
        </p>
        <p>
            Manifold learning assumes that high-dimensional data lies on a <strong>lower-dimensional, non-linear structure</strong> called a <strong>manifold</strong>. This means general patterns in high-dimensional data can often be represented in lower dimensions.
        </p>
    </div>

    <div class="content">
        <h2>2D <strong>t-SNE</strong> Visualization of 15 Clusters</h2>
        <p>
            A type of manifold learning technique is <strong>t-distributed Stochastic Neighbor Embedding (t-SNE)</strong>.
        </p>
        <ul>
          <li>
            <strong>t-distributed:</strong> Uses the <strong>Student's t-distribution</strong> with 1 degree of freedom (Cauchy distribution) 
            to model similarities between data points in the <strong>low-dimensional embedding space</strong>.
          </li>
          <li>
            <strong>Stochastic:</strong> Refers to the embedding process using <strong>stochastic gradient descent</strong> 
            to minimize the <strong>Kullback-Leibler divergence</strong> between the probabilistic models of the "membership strengths" of points in high and low dimensions.
          </li>
          <li>
            <strong>Neighbor embedding:</strong> Embeds local "neighborhoods" of data points faithfully into <strong>lower-dimensional graphs</strong>.
          </li>
        </ul>
        <p>
            We will now see how this algorithm works in <strong>practice</strong>.
        </p>
        <iframe src="plot_2d.html" width="100%" height="500" style="border:none;"></iframe>
    </div>

    <div class="content">
        <h2>High-level Overview of How <strong>t-SNE</strong> Works</h2>
        <p>
            Before diving into the maths, let's view a general overview of the process.
        </p>
        <h3>Manifolds</h3>
        <p>
            <strong>t-SNE</strong> and manifold learning techniques assume that high-dimensional data, though complex, actually lies on a <strong>lower-dimensional manifold</strong>. This simple sphere is an example of a manifold. 
        </p>
        <p>
        Here, the 3D sphere represents the high-dimensional geometry the global data lies on, and the "shell" or the surface of the sphere being the 2D manifold representing the lower-dimensional space it can be embedded onto. 
        </p>
        <p>
            Think of it as how we imagine Earth to be. Local neighborhoods seem to be on a flat plane, but when we fly to other countries, we take into account the true global geometry (spherical).
        </p>
        <iframe src="sphere.html" width="100%" height="500" style="border:none;"></iframe>
        <p>
            A manifold can be imagined as a shape in 3D space that locally looks like <strong>Euclidean space</strong> but has a more complicated global structure. Smooth connections between points are vital, with no sharp corners or edges.
        </p>
        <iframe src="manifoldcomplicated.html" width="100%" height="500" style="border:none;"></iframe>
        <p>
            Even complicated shapes like these are manifolds because the local geometry is still considered <strong>Euclidean</strong>. High-dimensional data often follows this principle.
        </p>
        <h3>Local Neighborhoods</h3>
        <p>
            Small, locally Euclidean neighborhoods, when stitched together, approximate the global geometry of the manifold. <strong>t-SNE</strong> analyzes these neighborhoods to create a lower-dimensional representation that preserves the overall structure of the data.
        </p>
    </div>

    <div class="content">
        <h2>The Mathematics of <strong>t-SNE</strong></h2>
        <p>
            <strong>t-SNE</strong> uses pairwise similarities as <strong>probabilistic models</strong> to represent "how likely a datapoint A is to choose datapoint B". By defining similarity probabilities in high dimensions, it becomes easier to apply <strong>mathematical computation</strong>.
        </p>
        <p>
            We also define a probabilistic model for the <strong>low-dimensional space</strong>, initializing points either randomly or using <strong>PCA</strong>. Similar models calculate similarity probabilities in low-D, which are initially different from high-D.
        </p>
        <p>
            Gradient descent is used to make the high-D and low-D probabilities similar. The formula used is the <strong>Kullback-Leibler Divergence (KL divergence)</strong>, which compares how different the two models are.
        </p>
    </div>

    <div class="content">
        <h2>Gradient Descent</h2>
        <p>
            Adjusting points in low-D minimizes <strong>KL divergence</strong>. Calculating the absolute minimum by moving every point is computationally expensive (O(NÂ²)), so <strong>stochastic gradient descent</strong> is used.
        </p>
        <p>
            Each point's position (<strong>yi</strong> and <strong>yj</strong>) in low-D is "nudged" to reduce KL divergence. Points too far from their high-D neighbors are pulled closer, and points too close are pushed apart.
        </p>
        <p>
            Over many iterations, these adjustments create a <strong>stable configuration</strong> where low-D distances reflect high-D neighborhood probabilities. The stochastic process allows t-SNE to handle large datasets efficiently while uncovering the underlying manifold structure.
        </p>
    </div>

    <div class="content">
        <h2>Watch this animation to see it in real time</h2>
        <iframe src="tsne_wine_animation.html" width="100%" height="500" style="border:none;"></iframe>
        <p>
            Using the <strong>wine dataset</strong> (3 wine types, 13 features), t-SNE reduces dimensions to two. Each batch of points improves the <strong>KL divergence</strong> loss, which may occasionally increase due to randomness.
        </p>
        <p>
            To visualize gradient descent, imagine a <strong>ball rolling on a bumpy surface</strong>, with the lowest point being the <strong>global minimum</strong>.
        </p>
        <iframe src="sgd_momentum_landscape.html" width="100%" height="500" style="border:none;"></iframe>
    </div>

</body>
</html>
