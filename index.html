<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dimensionality Reduction in Clustering</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 40px;
            background-color: #f9f9f9;
            color: #333;
        }
        .content {
            max-width: 800px;
            margin: auto;
            padding: 20px;
            background-color: #fff;
            border-radius: 10px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        h1 {
            text-align: center;
            color: #2c3e50;
        }
    </style>
</head>
<body>
        <h1>Applications of Gradient Descent: t-SNE Dimensionality Reduction</h1>
        <p>
            Coe Ishikawa, 2025
        </p>
        <h2>
            Introduction
        </h2>
        <p>
           This is an introductory article aimed at engineers who have a curiosity in where engineering maths (such as multivariate calculus) is used in rigorous mathematical research. Particularly for Design Engineers, dimensionality reduction will be crucial to analyse complex data and gather <strong>true hidden insights</strong>.
        </p>
        <p>
            <strong>High-Dimensional data</strong>, which is to say data with many features, is becoming more and more prevelant in today's fields of machine learning, computer vision and finance. These large datasets can provide <strong>invaluable insights</strong>, however as humans we are limited to visualising and identifying patterns in <strong>3 dimensions</strong>. This is where dimensionality reduction comes in.
        </p>
        <p>
            Dimensionality reduction techniques embed (project) datapoints from high dimensional space onto lower dimensional spaces, whilst retaining important relationships and clusters between datapoints.
        </p>
        <p>
            Many <strong>linear</strong> dimensionality reductions exist, the most popular and well known of which is Principal Component Analysis (PCA) which heavily relies on linear algebra to identify key axis (principal components) of variance. This topic is well covered by Devanshi in her report. 
        </p>
        <p>
            However, such techniques become less accurate when the data in question becomes non-linear (i.e. relationships between features cannot be explained by a simple line), and so special techniques such as <strong>t-SNE</strong> become necessary, based on <strong>manifold learning.</strong>
        </p>
    <div class="content">
        <h2>3D Visualization of 15 Clusters</h2>
        <p>
            First, we will understand why dimentionality reduction is important to find cluster relationships. 
            We shall start with a simple 3D graph with artificial clusters.
            *Hold and rotate the graph around to see the full data in 3D.
        </p>
        <iframe src="plot_3d.html" width="100%" height="500" style="border:none;"></iframe>
        <p>
            To simulate a very rudimentary dimentionality reduction, imagine the view of this
            graph on your 2D screen as a representation of the data in lower dimentions.
            In any angle, there are overlapping clusters and crucially, the "depth" distance
            between clusters is not preserved in this manner.  
            Furthermore, though I've made these clusters easy to see by laying it out in 3 dimensions, imagine a 4 dimensional space, or even 5. Try 100? It becomes utterly impossible and useless to try and visualise these dimensions. 
        </p>
        <p>
            Thus, we need a more robust algorithm that embeds high-D data with as much accuracy
            and faithfulness to the original topology of the data, in dimensions we can understand. We call this type of dimentionality 
            reduction techniques manifold learning techniques. 
        </p>
        <p>
            Manifold learning techniques assume high-dimentional data lies on a lower-dimentional,
            non-linear structure called a manifold. This means that the general patterns of any dimentionality data can be
            represented to a large extent with lower-dimensionality graphs. 
        </p>
    </div>
    <div class="content">
        <h2>2D t-SNE Visualization of 15 Clusters</h2>
        <p>
            A type of manifold learning techniques is called <strong>t-distributed Stochastic Neighbor Embedding (t-SNE).</strong>
        </p>
        <ul>
          <li>
            <strong>t-distributed:</strong> Uses the Student's t-distribution with 1 degree of freedom (Cauchy distribution) 
            to model the similarities between data points in the low-dimensional embedding space.
          </li>
          <li>
            <strong>Stochastic:</strong> Refers to the nature of the embedding process using stochastic gradient descent 
            to minimize the Kullback-Leibler divergence between the probabilistic models of the "membership strengths" 
            of data points in high and low dimensions.
          </li>
          <li>
            <strong>Neighbor embedding:</strong> Refers to t-SNE's approach of faithfully embedding local "neighborhoods" 
            of data points into lower-dimensional graphs.
          </li>
        </ul>
        <p>
            We will now see how this algorithm works in practise
        </p>
        <iframe src="plot_2d.html" width="100%" height="500" style="border:none;"></iframe>
    </div>
    <div class="content">
        <h2>High level overview of how t-SNE works</h2>
        

        <p>
            Before we dive into the maths behind this fascinating machine learning algorithm, we must first view a general overview of the process.
        </p>
        <h3>Manifolds</h3>
        <p>
            t-SNE, and all manifold learning techniques, are based on the assumption 
            that all high dimensional data, though daunting with its hundreds of dimensionalities, 
            actually lie on a much lower dimension or manifold. This simple sphere is an example of a manifold.
        </p>
        <iframe src="sphere.html" width="100%" height="500" style="border:none;"></iframe>
        <p>
            A manifold can be imagined like a shape in 3d space, 
            that locally looks like euclidean space (geometry of a flat surface), but its global structure is more 
            complicated. There are a few rules to this, which are vital to allowing for smooth,
            uninterrupted connections between neighbor data points to be calculated, the most 
            important of which is that there are no sharp corners or edges to the manifold geometry.
        </p>
            <iframe src="manifoldcomplicated.html" width="100%" height="500" style="border:none;"></iframe>
        <p>
            This means that even complicated shapes like this is still a manifold as the local geometry
            when you zoom into the surface is still considered euclidean. In reality, the global geometry
            of data in high dimentions become much more complicated, but its shape shall be assumed to
            follow the rules of a manifold.
        </p>
        <h3>
            Small, locally Euclidean neighborhoods, when stitched together, approximate the global geometry of the manifold.       
        </h3>
        <p>
        t-SNE and similar algorithms work by analyzing the relationships between data points within small local neighborhoods. This is similar to breaking down handwritten digits into basic strokes and shapes, where the algorithm focuses on how each part relates to its nearby patterns. By preserving these local relationships, t-SNE creates a lower-dimensional representation that captures the overall structure of the data.
        </p>
        <h2>
            The Mathematics of t-SNE
        </h2>
        <p>
            Now that we understand manifolds, we can see how t-SNE uses neighborhood relationships to faithfully represent high dimentionality data in lower dimentional space. For this, t-SNE uses pairwise similarities as probabilistic models to represent "how likely a datapoint A is to choose datapoint B". By defining a similarity probability for every pair in high dimensions, it becomes much easier to apply mathematical computation.
        </p>
        <p>
            Now we need a similar probabilistic model for our low-dimensional graph, which datapoints are initialized either randomly or using PCA (see Devanshi's work). Like we did with high dimensional data, similar probabilistic models are used to calculate a similarity probability for each pair in low-D (which at first will be completely different to what it should be in high-D).
        </p>
        <p>
            Here is where gradient descent from your lectures in multivariate calculus comes into play. To make the two probabilities between datapoints in high-D similar to low-D, t-SNE uses a formula called the Kullback-Leiber Divergence (KL divergence), essentially comparing how different model A is to model B (Model A being the high-D pairwise similarities and model B being the same for low-D).
        </p>
        <h2>
            Gradient Descent
        </h2>
        <p>
            Adjusting the datapoints in low-D now becomes the next logical step to minimise the KL divergence, however calculating the absolute minimal value by moving every datapoint (which there may be millions) will be compuationally taxing O(N^2). Therefore, t-SNE uses a type of gradient descent called stochastic gradient descent.
        </p>
        <p>
            The parameter being adjusted in each iteration is the position of each datapoint yi and yj in the low-dimensional space. Each point is “nudged” slightly in the direction that reduces the KL divergence: points that are too far from their high-dimensional neighbors are pulled closer, while points that are too close are pushed apart.

            Over many iterations, these small adjustments gradually lead to a stable configuration where the low-dimensional distances reflect the high-dimensional neighborhood probabilities as faithfully as possible. The stochastic nature of the process allows t-SNE to handle large datasets efficiently while still uncovering the underlying manifold structure in a visually meaningful way.
        </p>
        <h2>
            Watch this animation to see how this works in real time
        </h2>
        <iframe src="tsne_wine_animation.html" width="100%" height="500" style="border:none;"></iframe>
        <h2>
            Explaination
        </h2>
        <p>
            Using the wine dataset that classifies 3 types of wine based on 13 features. To find relationships between these 3 types of wine (colored separately), t-SNE reduces the dimensions to two. Each batch of points chosen and moved makes improvements in the KL divergence (loss) albeit at times this loss increases due to the random nature of the sampling process.
        </p>
        <p>
            To intuitively visualise this in gradient descent, we can imagine a ball rolling across a bumpy surface, the lowest point being the global minima 
        </p>
        <iframe src="sgd_momentum_landscape.html" width="100%" height="500" style="border:none;"></iframe>
        <p>
            As you can see, the loss increases when it moves towards a global minima, which can be seen in the t-SNE of the wine dataset.
        </p>

</body>

</html>
