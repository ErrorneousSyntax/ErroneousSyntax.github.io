<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dimensionality Reduction in Clustering</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 40px;
            background-color: #f9f9f9;
            color: #333;
        }
        .content {
            max-width: 800px;
            margin: auto;
            padding: 20px;
            background-color: #fff;
            border-radius: 10px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        h1 {
            text-align: center;
            color: #2c3e50;
        }
    </style>
</head>
<body>
    <div class="content">
        <h1>Dimensionality Reduction to identify patterns in the high-dimensionality data</h1>
        <p>
            For those of you in DE1, taking the engineering mathematics module, I will explain how 
            the "abstract maths" you are learning now is applied very heavily even in rigorous recent research in 
            data science, hopefully fueling your interest in the subject. I will introduce an advanced topic called 
            dimentionality reduction, which can be vital as Design Engineers to identify patterns and 
            relationships in data to make accurate, informed decisions.
        </p>
        <p>
            In data science, we often attempt to find insights from big data, looking for
            hidden patterns and relationships. However, with datasets containing more than 3 
            features, one must consider a graph of high-dimensions (such as a graph with 100 Dimensionality).
            The human mind simply cannot comprehend this, yet visualising this sort
            of data is becoming increasingly important to uncover hidden insights. 
        </p>
    <div class="content">
        <h2>3D Visualization of 15 Clusters</h2>
        <p>
            To attempt to understand why dimentionality reduction to find clusters is important, 
            we shall start with a simple 3d graph with artificial clusters. (Note that it is easy
            to identify clusters in low Dimensionality as seen here, but is impossible beyond 4 dimensions.)
            *Hold and rotate the graph around to see the full data in 3D.
        </p>
        <iframe src="plot_3d.html" width="100%" height="500" style="border:none;"></iframe>
        <p>
            To simulate a very rudimentary dimentionality reduction, imagine the view of this
            graph on your 2D screen is a representation of the data in lower dimentions.
            In any angle, there are overlapping clusters, and even more evidently, the "depth" distance
            between clusters is not preserved in this manner. 
        </p>
        <p>
            Thus, we need a more robust algorithm that embeds high-D data with as much accuracy
            and faithfulness to the original topology of the data. We call this type of dimentionality 
            reduction techniques manifold learning techniques. 
        </p>
        <p>
            Manifold learning techniques assume high-dimentional data lies on a lower-dimentional,
            non-linear structure called a manifold. This means that the general patterns of any dimentionality data can be
            represented to a large extent with lower-dimensionality graphs. 
        </p>
    </div>
    <div class="content">
        <h2>2D t-SNE Visualization of 15 Clusters</h2>
        <p>
            A type of manifold learning techniques is called t-distributed Stochastic Neighbor Embedding.
            t-distributed since it uses the student's t-distribution with 1 degree of freedom (Cauchy distribution)
            to model the similiarities between data points in the low-dimensional embedding space. Stochastic referring
            to the nature of the embedding process using stochastic gradient descent to minimise the Kullback-Leiber divergence between
            the probabilistic models of the "membership strengths" between datapoints in high dimensions and
            low dimensions. Finally Neighbor embedding refers to the nature of t-SNE which embeds local "neighborhoods"
            of datapoints very faithfully into lower dimension graphs. We will explain each step in more detail further down this article.
        </p>
        <iframe src="plot_2d.html" width="100%" height="500" style="border:none;"></iframe>
    </div>
    <div class="content">
        <h2>High level overview of how t-SNE works</h2>
        <p>
            Before we dive into the maths behind this fascinating machine learning algorithm, we must first view a general overview of the process.
        </p>
        <p>
            t-SNE, and all manifold learning techniques, are based on the assumption 
            that all high dimensional data, though daunting with its hundreds of dimensionalities, 
            actually lie on a much lower dimension or manifold. A manifold can be imagined like a piece
             of paper that has been bent and folded in many convoluted ways in space. 
        </p>

            <iframe src="manifold.html" width="100%" height="500" style="border:none;"></iframe>
        </p>
    </div>
</body>

</html>
